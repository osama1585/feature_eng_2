{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf5b657d-1c14-4e92-aa66-95ff33b1895a",
   "metadata": {},
   "source": [
    "<span style=color:red;font-size:55px>ASSIGNMENT</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a26ec39-9d4a-4159-bce2-96935b1b5ebd",
   "metadata": {},
   "source": [
    "<span style=color:pink;font-size:50px>FEATURE ENGINEERING-2</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbe9ed0-7860-4ffa-b44b-ff877cd5ce9e",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate itsapplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df188776-93f7-4e98-97ec-8dd1f4d5e03b",
   "metadata": {},
   "source": [
    "## Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d83141-4192-4d1e-97af-ef93ecaa2906",
   "metadata": {},
   "source": [
    "## Min-Max Scaling in Data Preprocessing\n",
    "\n",
    "Min-Max scaling is a data normalization technique used to rescale numerical features to a fixed range, typically between 0 and 1. It transforms the data in such a way that the minimum value of the feature becomes 0, the maximum value becomes 1, and all other values are scaled proportionally within this range.\n",
    "\n",
    "### How Min-Max Scaling Works\n",
    "\n",
    "- **Formula**:\n",
    "  - For each feature \\( x \\):\n",
    "    - \\( x_{\\text{scaled}} = \\frac{{x - \\text{min}(x)}}{{\\text{max}(x) - \\text{min}(x)}} \\)\n",
    "\n",
    "- **Range**:\n",
    "  - Scaled values are between 0 and 1.\n",
    "  - If \\( x = \\text{min}(x) \\), then \\( x_{\\text{scaled}} = 0 \\).\n",
    "  - If \\( x = \\text{max}(x) \\), then \\( x_{\\text{scaled}} = 1 \\).\n",
    "\n",
    "### Example Application\n",
    "\n",
    "Suppose we have a dataset containing the following numerical feature representing the age of individuals:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc52e3c-d118-4dd4-9d95-836334afe0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[25]\n",
      " [30]\n",
      " [40]\n",
      " [35]\n",
      " [28]]\n",
      "\n",
      "Scaled Data:\n",
      "[[0.        ]\n",
      " [0.33333333]\n",
      " [1.        ]\n",
      " [0.66666667]\n",
      " [0.2       ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "data = np.array([[25], [30], [40], [35], [28]])\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit scaler to the data and transform\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Print original and scaled data\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nScaled Data:\")\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf6a00-e953-45a3-bf8f-ea88b12d0db7",
   "metadata": {},
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d4932c-9c88-4fd7-97bb-4ca4846188eb",
   "metadata": {},
   "source": [
    "## Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f7ffde-0d86-40b4-8f13-686b441a340b",
   "metadata": {},
   "source": [
    "## Unit Vector Scaling in Feature Scaling\n",
    "\n",
    "Unit Vector scaling, also known as normalization, is a technique used to rescale numerical features to have unit norm, i.e., their magnitude becomes 1. It transforms the data in such a way that each sample (row) in the dataset has a Euclidean norm of 1.\n",
    "\n",
    "### How Unit Vector Scaling Works\n",
    "\n",
    "- **Formula**:\n",
    "  - For each feature \\( x \\) in a sample:\n",
    "    - \\( x_{\\text{scaled}} = \\frac{x}{\\|x\\|} \\)\n",
    "\n",
    "- **Normalization**:\n",
    "  - Scaled values are adjusted such that the Euclidean norm (magnitude) of each sample becomes 1.\n",
    "\n",
    "### Example Application\n",
    "\n",
    "Suppose we have a dataset containing two numerical features representing the length and width of objects:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7b3a724-7c1d-40c3-be26-c5053e763b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[3 4]\n",
      " [1 2]\n",
      " [4 6]]\n",
      "\n",
      "Scaled Data:\n",
      "[[0.6        0.8       ]\n",
      " [0.4472136  0.89442719]\n",
      " [0.5547002  0.83205029]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "data = np.array([[3, 4], [1, 2], [4, 6]])\n",
    "\n",
    "# Calculate Euclidean norms for each sample\n",
    "norms = np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# Apply Unit Vector scaling\n",
    "scaled_data = data / norms\n",
    "\n",
    "# Print original and scaled data\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nScaled Data:\")\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1628d031-76c8-4bea-9317-1da495b22445",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9c293a-95c8-4f18-ab19-470e4b202dcc",
   "metadata": {},
   "source": [
    "## Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7bce44-4272-4dbc-a93d-13d464e460d7",
   "metadata": {},
   "source": [
    "## PCA (Principal Component Analysis) for Dimensionality Reduction\n",
    "\n",
    "PCA, or Principal Component Analysis, is a popular technique used for dimensionality reduction in machine learning and data analysis. It transforms high-dimensional data into a lower-dimensional representation by identifying the directions, or principal components, that capture the maximum variance in the data. These principal components are orthogonal to each other and are ordered by the amount of variance they explain.\n",
    "\n",
    "### How PCA Works\n",
    "\n",
    "1. **Compute Covariance Matrix**:\n",
    "   - Calculate the covariance matrix of the original data.\n",
    "\n",
    "2. **Eigenvalue Decomposition**:\n",
    "   - Perform eigenvalue decomposition (or Singular Value Decomposition) on the covariance matrix to obtain eigenvectors and eigenvalues.\n",
    "\n",
    "3. **Select Principal Components**:\n",
    "   - Sort the eigenvectors by their corresponding eigenvalues in descending order.\n",
    "   - Select the top \\( k \\) eigenvectors (principal components) that explain the most variance in the data, where \\( k \\) is the desired dimensionality of the reduced data.\n",
    "\n",
    "4. **Transform Data**:\n",
    "   - Project the original data onto the selected principal components to obtain the lower-dimensional representation.\n",
    "\n",
    "### Example Application\n",
    "\n",
    "Suppose we have a dataset containing two numerical features representing the height and weight of individuals:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5091afe4-fc4d-42f0-bb58-930880da3883",
   "metadata": {},
   "source": [
    "\n",
    "To apply PCA to this dataset:\n",
    "\n",
    "1. **Compute Covariance Matrix**:\n",
    "   - Calculate the covariance matrix of the original data.\n",
    "\n",
    "2. **Eigenvalue Decomposition**:\n",
    "   - Perform eigenvalue decomposition on the covariance matrix to obtain eigenvectors and eigenvalues.\n",
    "\n",
    "3. **Select Principal Components**:\n",
    "   - Select the eigenvector corresponding to the highest eigenvalue as the first principal component, and the second-highest eigenvalue as the second principal component.\n",
    "\n",
    "4. **Transform Data**:\n",
    "   - Project the original data onto the selected principal components to obtain the lower-dimensional representation.\n",
    "\n",
    "### Benefits of PCA for Dimensionality Reduction\n",
    "\n",
    "- **Dimensionality Reduction**: PCA helps reduce the number of features (dimensions) in the data while preserving as much variance as possible.\n",
    "- **Feature Interpretability**: The principal components can often be interpreted in terms of the original features, providing insights into the underlying structure of the data.\n",
    "- **Computational Efficiency**: PCA simplifies the data representation, making it computationally more efficient for subsequent analysis and modeling.\n",
    "\n",
    "PCA is widely used in various fields, including image processing, genetics, finance, and natural language processing, for reducing the dimensionality of high-dimensional datasets while retaining essential information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423dde7d-0d1f-45a7-8d35-f1ea414628b9",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9940f719-a430-454e-85be-b179c4e8c8bc",
   "metadata": {},
   "source": [
    "## Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cb498d-2667-4fb2-b2b0-80acd049b6ab",
   "metadata": {},
   "source": [
    "## Relationship between PCA and Feature Extraction\n",
    "\n",
    "PCA (Principal Component Analysis) can be used for feature extraction, especially in cases where the original features are highly correlated or redundant. Feature extraction involves transforming the original features into a new set of features (or components) that capture the most important information in the data.\n",
    "\n",
    "### How PCA Works for Feature Extraction\n",
    "\n",
    "1. **Dimensionality Reduction**:\n",
    "   - PCA identifies the directions (principal components) in the feature space that capture the maximum variance in the data.\n",
    "\n",
    "2. **Feature Transformation**:\n",
    "   - The original features are projected onto the principal components, effectively transforming the data into a lower-dimensional space.\n",
    "\n",
    "3. **Feature Extraction**:\n",
    "   - The principal components themselves serve as the extracted features, representing combinations of the original features that explain the most variance in the data.\n",
    "\n",
    "### Example Application\n",
    "\n",
    "Suppose we have a dataset containing three numerical features representing different aspects of a product:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7f75f7-8f84-45ad-8e64-c3935149197c",
   "metadata": {},
   "source": [
    "\n",
    "To apply PCA for feature extraction to this dataset:\n",
    "\n",
    "1. **Standardize Features**:\n",
    "   - Standardize the features by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "2. **Perform PCA**:\n",
    "   - Use PCA to identify the principal components that capture the most variance in the standardized data.\n",
    "\n",
    "3. **Select Principal Components**:\n",
    "   - Select the top \\( k \\) principal components that explain the desired amount of variance (e.g., 95%).\n",
    "\n",
    "4. **Transform Data**:\n",
    "   - Project the original data onto the selected principal components to obtain the lower-dimensional representation.\n",
    "\n",
    "### Benefits of PCA for Feature Extraction\n",
    "\n",
    "- **Dimensionality Reduction**: PCA reduces the dimensionality of the data by transforming it into a smaller set of principal components.\n",
    "- **Noise Reduction**: PCA can help remove noise and irrelevant information from the data, focusing on the most significant features.\n",
    "- **Interpretability**: The principal components can often be interpreted in terms of the original features, providing insights into the underlying structure of the data.\n",
    "\n",
    "PCA for feature extraction is widely used in various domains, including image processing, signal processing, and natural language processing, to extract meaningful features from high-dimensional datasets while reducing computational complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a390c-4fe4-42d8-8559-78c1202bedf6",
   "metadata": {},
   "source": [
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e15809-b157-4ae8-bc81-03a1d2f21d59",
   "metadata": {},
   "source": [
    "## Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd502f3e-a65a-45a4-92c6-c791c2e657f2",
   "metadata": {},
   "source": [
    "## Preprocessing Data for a Food Delivery Recommendation System Using Min-Max Scaling\n",
    "\n",
    "When working on a project to build a recommendation system for a food delivery service, preprocessing the dataset is essential to ensure that the features are on a similar scale. Min-Max scaling is a common technique used to rescale numerical features to a fixed range, typically between 0 and 1. Here's how you could use Min-Max scaling to preprocess the data:\n",
    "\n",
    "1. **Identify Numerical Features**:\n",
    "   - Review the dataset to identify numerical features such as price, rating, and delivery time.\n",
    "\n",
    "2. **Compute Min and Max Values**:\n",
    "   - For each numerical feature, calculate the minimum and maximum values in the dataset. This step helps determine the range to which the features will be scaled.\n",
    "\n",
    "3. **Apply Min-Max Scaling**:\n",
    "   - For each numerical feature \\( x \\), apply the Min-Max scaling formula:\n",
    "     - \\( x_{\\text{scaled}} = \\frac{{x - \\text{min}(x)}}{{\\text{max}(x) - \\text{min}(x)}} \\)\n",
    "   - This formula rescales each feature to a range between 0 and 1, where the minimum value becomes 0, and the maximum value becomes 1.\n",
    "\n",
    "4. **Transform Data**:\n",
    "   - Apply the Min-Max scaling transformation to all numerical features in the dataset.\n",
    "\n",
    "5. **Normalized Dataset**:\n",
    "   - After preprocessing, the dataset will contain the same features but with values scaled to a common range, making it suitable for modeling and analysis.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Suppose we have a dataset with the following numerical features:\n",
    "\n",
    "- Price: ranging from $5 to $20\n",
    "- Rating: ranging from 3 to 5\n",
    "- Delivery Time: ranging from 15 to 45 minutes\n",
    "\n",
    "To preprocess this dataset using Min-Max scaling:\n",
    "\n",
    "- Price (scaled) = \\( \\frac{{\\text{Price} - 5}}{{20 - 5}} \\)\n",
    "- Rating (scaled) = \\( \\frac{{\\text{Rating} - 3}}{{5 - 3}} \\)\n",
    "- Delivery Time (scaled) = \\( \\frac{{\\text{Delivery Time} - 15}}{{45 - 15}} \\)\n",
    "\n",
    "After applying Min-Max scaling, all numerical features will be transformed to a range between 0 and 1, ensuring uniformity in scale across different features.\n",
    "\n",
    "By using Min-Max scaling to preprocess the data for the food delivery recommendation system, we ensure that the features are on a similar scale, which can improve the performance of machine learning models and recommendation algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888255cf-8a5c-4939-a17b-4f0e7f96c22e",
   "metadata": {},
   "source": [
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea132a7-0abe-4bf6-a845-d66e353353f6",
   "metadata": {},
   "source": [
    "## Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0505241-ca90-4096-b3e0-6bedeb22b3b1",
   "metadata": {},
   "source": [
    "## Using PCA for Dimensionality Reduction in Stock Price Prediction\n",
    "\n",
    "When working on a project to predict stock prices, dealing with a dataset containing numerous features, including company financial data and market trends, can lead to high dimensionality. PCA (Principal Component Analysis) can be a valuable technique to reduce the dimensionality of such datasets while preserving essential information. Here's how you could use PCA for dimensionality reduction:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - Standardize the dataset: It's crucial to standardize the features to have a mean of 0 and a standard deviation of 1 before applying PCA. This ensures that all features contribute equally to the analysis.\n",
    "\n",
    "2. **Compute Covariance Matrix**:\n",
    "   - Calculate the covariance matrix of the standardized dataset. The covariance matrix represents the relationships between different features, providing insights into their correlations.\n",
    "\n",
    "3. **Perform PCA**:\n",
    "   - Apply PCA to the covariance matrix to obtain the principal components. PCA will identify the directions in the feature space that capture the maximum variance in the data.\n",
    "\n",
    "4. **Select Principal Components**:\n",
    "   - Determine the number of principal components to retain based on the desired level of variance explained. You can choose the number of principal components that collectively explain a significant portion of the total variance in the dataset (e.g., 95%).\n",
    "\n",
    "5. **Transform Data**:\n",
    "   - Project the original dataset onto the selected principal components. This transformation results in a lower-dimensional representation of the dataset, with fewer features capturing the most significant variance.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Suppose we have a dataset containing various features related to stock performance, such as earnings per share (EPS), price-to-earnings ratio (P/E), market capitalization, and industry sector indicators.\n",
    "\n",
    "To apply PCA for dimensionality reduction to this dataset:\n",
    "\n",
    "- Standardize the features to have a mean of 0 and a standard deviation of 1.\n",
    "- Compute the covariance matrix of the standardized dataset.\n",
    "- Perform PCA on the covariance matrix to obtain the principal components.\n",
    "- Select the top \\( k \\) principal components that collectively explain a significant portion of the total variance (e.g., 95%).\n",
    "- Project the original dataset onto the selected principal components to obtain the lower-dimensional representation.\n",
    "\n",
    "After applying PCA, the dataset will contain a reduced number of principal components capturing the most important information about stock performance, making it more manageable for modeling while retaining relevant features for predicting stock prices.\n",
    "\n",
    "By using PCA for dimensionality reduction in stock price prediction, you can effectively handle high-dimensional datasets and improve the efficiency and interpretability of machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd79f6d-2495-49e6-afcc-51d0ca0fdfac",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c66eae-2656-4fe5-bad0-ed124a93e0f9",
   "metadata": {},
   "source": [
    "## Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1e32e-0afc-4bba-8690-e566ed374d02",
   "metadata": {},
   "source": [
    "## Min-Max Scaling to Transform Values to a Range of -1 to 1\n",
    "\n",
    "Given the dataset: [1, 5, 10, 15, 20]\n",
    "\n",
    "### Min-Max Scaling Formula:\n",
    "\n",
    "To scale the values to a range of -1 to 1, we can use the following formula:\n",
    "\n",
    "\\[ x_{\\text{scaled}} = \\frac{{x - \\text{min}(x)}}{{\\text{max}(x) - \\text{min}(x)}} \\times (b - a) + a \\]\n",
    "\n",
    "Where:\n",
    "- \\( x \\) is the original value.\n",
    "- \\( \\text{min}(x) \\) is the minimum value in the dataset.\n",
    "- \\( \\text{max}(x) \\) is the maximum value in the dataset.\n",
    "- \\( a \\) and \\( b \\) are the desired minimum and maximum values of the scaled range, respectively.\n",
    "\n",
    "### Applying Min-Max Scaling:\n",
    "\n",
    "Given the dataset: [1, 5, 10, 15, 20]\n",
    "\n",
    "- \\( \\text{min}(x) = 1 \\)\n",
    "- \\( \\text{max}(x) = 20 \\)\n",
    "- \\( a = -1 \\)\n",
    "- \\( b = 1 \\)\n",
    "\n",
    "Using the Min-Max scaling formula, we can compute the scaled values as follows:\n",
    "\n",
    "1. For \\( x = 1 \\):\n",
    "   \\[ x_{\\text{scaled}} = \\frac{{1 - 1}}{{20 - 1}} \\times (1 - (-1)) + (-1) = 0 \\times 2 - 1 = -1 \\]\n",
    "\n",
    "2. For \\( x = 5 \\):\n",
    "   \\[ x_{\\text{scaled}} = \\frac{{5 - 1}}{{20 - 1}} \\times (1 - (-1)) + (-1) = \\frac{4}{19} \\times 2 - 1 \\approx -0.8421 \\]\n",
    "\n",
    "3. For \\( x = 10 \\):\n",
    "   \\[ x_{\\text{scaled}} = \\frac{{10 - 1}}{{20 - 1}} \\times (1 - (-1)) + (-1) = \\frac{9}{19} \\times 2 - 1 \\approx -0.3684 \\]\n",
    "\n",
    "4. For \\( x = 15 \\):\n",
    "   \\[ x_{\\text{scaled}} = \\frac{{15 - 1}}{{20 - 1}} \\times (1 - (-1)) + (-1) = \\frac{14}{19} \\times 2 - 1 \\approx 0.1053 \\]\n",
    "\n",
    "5. For \\( x = 20 \\):\n",
    "   \\[ x_{\\text{scaled}} = \\frac{{20 - 1}}{{20 - 1}} \\times (1 - (-1)) + (-1) = 1 \\times 2 - 1 = 1 \\]\n",
    "\n",
    "### Scaled Values:\n",
    "\n",
    "The scaled values of the dataset [1, 5, 10, 15, 20] in the range of -1 to 1 are approximately:\n",
    "\n",
    "\\[ [-1, -0.8421, -0.3684, 0.1053, 1] \\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9200bc8a-8762-4255-9060-d0152259a15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Values:\n",
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Define the desired range\n",
    "a = -1\n",
    "b = 1\n",
    "\n",
    "# Perform Min-Max scaling\n",
    "min_val = np.min(data)\n",
    "max_val = np.max(data)\n",
    "scaled_data = (data - min_val) / (max_val - min_val) * (b - a) + a\n",
    "\n",
    "# Print the scaled values\n",
    "print(\"Scaled Values:\")\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ff27f-ebb2-497a-b1f7-1a898787163c",
   "metadata": {},
   "source": [
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da666f6-e5b8-42d3-bad0-d2070d9780c7",
   "metadata": {},
   "source": [
    "## Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6142fe-8a45-4cb4-af26-9c0e7106f0fc",
   "metadata": {},
   "source": [
    "## Feature Extraction Using PCA for the Given Dataset\n",
    "\n",
    "When performing feature extraction using PCA (Principal Component Analysis), it's essential to determine the number of principal components to retain. Here's how you can approach this for the given dataset containing features: [height, weight, age, gender, blood pressure].\n",
    "\n",
    "### Steps for Feature Extraction Using PCA:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - Standardize the dataset: Ensure that the features have a mean of 0 and a standard deviation of 1 before applying PCA.\n",
    "\n",
    "2. **Perform PCA**:\n",
    "   - Apply PCA to the standardized dataset to obtain the principal components.\n",
    "\n",
    "3. **Determine the Number of Principal Components**:\n",
    "   - Assess the cumulative explained variance ratio to decide how many principal components to retain. You can choose a threshold value (e.g., 95%) and select the minimum number of principal components that collectively explain this percentage of variance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce9ef3a5-faae-4122-9177-503907254e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Principal Components to Retain: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Given dataset features: [height, weight, age, gender, blood pressure]\n",
    "data = np.array([\n",
    "    [170, 68, 30, 1, 120],\n",
    "    [165, 55, 25, 0, 110],\n",
    "    [180, 75, 35, 1, 130],\n",
    "    [160, 50, 40, 0, 115],\n",
    "    # Add more data as needed\n",
    "])\n",
    "\n",
    "# Standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "# Assess cumulative explained variance ratio\n",
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Determine the number of principal components to retain\n",
    "threshold_variance = 0.95  # Choose a threshold (e.g., 95%)\n",
    "num_components = np.argmax(cumulative_variance_ratio >= threshold_variance) + 1\n",
    "\n",
    "# Print the number of principal components to retain\n",
    "print(\"Number of Principal Components to Retain:\", num_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc395b7-c6e8-4483-91ad-358c6b042dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
